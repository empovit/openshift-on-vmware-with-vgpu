---
- name: Single-Node OpenShift (SNO) with NVIDIA vGPU on VMware
  hosts: localhost
  gather_facts: no

  roles:
    - karmab.ansible_aicli_modules

  vars:
    equinix_metal_git_repo: https://github.com/empovit/equinix-metal-vsphere
    equinix_metal_git_branch: all-fixes
    equinix_metal_tf_dir: equinix_metal
    esxi_update_filename: "ESXi-7.0U3d-19482537-standard"
    vcenter_iso_name: "VMware-VCSA-all-7.0.3-19480866.iso"
    vcenter_username: "Administrator@vsphere.local"
    vib_filename: "NVD-AIE_510.47.03-1OEM.702.0.0.17630552_19298122.zip"
    vgpu_profile: "undefined" # For supported profiles run `vim-cmd hostsvc/hostconfig` on ESXi and scroll down to `sharedPassthruGpuTypes`
    ssh_public_key: "~/.ssh/id_rsa.pub"
    cluster_name: sno-vmware
    openshift_version: "4.10.11"
    install_vgpu_driver: true
    create_vm: true

  tasks:

    - name: Install Python prerequisites
      ansible.builtin.pip:
        name:
          - PyVmomi
          - aicli
          - openshift>=0.6
          - PyYAML >= 3.11
          - kubernetes-validate

    - name: Create temp directory
      ansible.builtin.file:
        path: tmp
        state: directory

    - name: Clone vSphere installation scripts
      ansible.builtin.git:
        repo: "{{ equinix_metal_git_repo }}"
        dest: "{{ playbook_dir }}/{{ equinix_metal_tf_dir }}"
        version: "{{ equinix_metal_git_branch }}"
        single_branch: yes

    - name: Generate Terraform variables
      ansible.builtin.template:
        src: templates/terraform.tfvars.j2
        dest: "tmp/terraform.tfvars"

    - name: Install vSphere on Equinix Metal using Terraform
      community.general.terraform:
        project_path: "{{ playbook_dir }}/{{ equinix_metal_tf_dir }}"
        variables_files: tmp/terraform.tfvars
        state: present
        force_init: true
      register: vsphere_cluster

    - name: Set vCenter connection variables
      ansible.builtin.set_fact:
        vcenter_ip: "{{ vsphere_cluster.outputs.vcenter_ip.value }}"
        vcenter_username: "{{ vsphere_cluster.outputs.vcenter_username.value }}"
        vcenter_password: "{{ vsphere_cluster.outputs.vcenter_password.value }}"

    - name: Add bastion host to the inventory
      ansible.builtin.add_host:
        hostname: bastion
        ansible_host: "{{ vsphere_cluster.outputs.bastion_host.value }}"
        ansible_user: root

    - ansible.builtin.debug:
        msg:
          - "Bastion host: {{ hostvars.bastion.ansible_host }}"
          - "vCenter IP: {{ vcenter_ip }}"
          - "vCenter username: {{ vcenter_username }}"
          - "vCenter password: {{ vcenter_password }}"

    - name: Add bastion's fingerprint on first-time SSH connection
      connection: local
      ansible.builtin.shell: |
        ssh-keygen -F {{ hostvars.bastion.ansible_host }} ||
        ssh-keyscan -H {{ hostvars.bastion.ansible_host }} >> ~/.ssh/known_hosts
      register: known_hosts_update
      changed_when: "'found' not in known_hosts_update.stdout"

    - name: Copy script for collecting ESXi host addresses
      ansible.builtin.copy:
        src: esxi_hosts.py
        dest: bootstrap/esxi_hosts.py
        mode: u=xwr,g=r,o=r
      delegate_to: bastion

    - name: Copy script for downloading assisted installer ISO
      ansible.builtin.copy:
        src: download_iso.py
        dest: bootstrap/download_iso.py
        mode: u=xwr,g=r,o=r
      delegate_to: bastion

    - name: Download vGPU VIB from S3 storage
      ansible.builtin.command: "mc cp s3/{{ s3_bucket }}/{{ vib_filename }} ."
      args:
        chdir: bootstrap
      delegate_to: bastion
      when: install_vgpu_driver

    - name: Copy script for installing vGPU on all remote ESXi hosts
      ansible.builtin.template:
        src: templates/install_vgpu.py.j2
        dest: bootstrap/install_vgpu.py
        mode: u=xwr,g=r,o=r
      delegate_to: bastion
      when: install_vgpu_driver

    - name: Copy script for locally installing VIB on an ESXi host
      ansible.builtin.template:
        src: templates/install_vib_esxi.sh.j2
        dest: bootstrap/install_vib_esxi.sh
        mode: u=xwr,g=r,o=r
      delegate_to: bastion
      when: install_vgpu_driver

    - name: Install vGPU host driver on all ESXi hosts
      ansible.builtin.command: "python3 $HOME/bootstrap/install_vgpu.py"
      args:
        chdir: bootstrap
      delegate_to: bastion
      when: install_vgpu_driver

    - name: Wait for vCenter to become available
      ansible.builtin.uri:
        url: "https://{{ vcenter_ip }}/ui/"
        method: GET
        validate_certs: no
      register: _result
      until: _result.status == 200
      retries: 20
      delay: 60

    # To change host graphics to "Shared Direct" programmatically,
    # see https://github.com/ailaunchpad/ailp-nvidia-ai-enterprise/blob/30ada15b5a0d5e65824a393b2a9eda743da425bb/modules/equinix/metal/vcva/templates/shared_direct.py
    # Keep in mind that simply restarting Xorg might not work, and reboot may be required

    # - name: Host graphics for vGPU setup
    #   community.vmware.vmware_host_config_manager:
    # hostname: "{{ vcenter_ip }}"
    # username: "{{ vcenter_username }}"
    # password: "{{ vcenter_password }}"
    #     esxi_hostname: "{{ esxi_hostname }}"
    #     # the list of advanced options can be found by running `vim-cmd hostsvc/advopt/options` on ESXi host
    #     options:
    #         'Config.Hardware.graphics.': false

    - name: Create assisted SNO cluster
      ai_cluster:
        name: "{{ cluster_name }}"
        state: present
        parameters:
          openshift_version: "{{ openshift_version }}"
          sno: true
          minimal: true
          pull_secret: "{{ pull_secret_path }}"
          base_dns_domain: "{{ openshift_base_domain }}"
          ssh_public_key: "{{ lookup('file', ssh_public_key) }}"
        offlinetoken: "{{ ocm_offline_token }}"

    - name: Read SNO cluster
      ai_cluster_info:
        name: "{{ cluster_name }}"
        offlinetoken: "{{ ocm_offline_token }}"
      register: sno_cluster

    - name: Read SNO cluster's InfraEnv
      ai_infraenv_info:
        name: "{{ cluster_name }}"
        offlinetoken: "{{ ocm_offline_token }}"
      register: sno_infraenv

    - name: Download assisted installer ISO for SNO cluster
      ansible.builtin.command: "python3 $HOME/bootstrap/download_iso.py {{ sno_infraenv.download_url }} {{ cluster_name }}.iso"
      args:
        chdir: bootstrap
      delegate_to: bastion
      when: create_vm

    # To add a vGPU as PCI device programmatically, see:
    # https://github.com/ansible-collections/community.vmware/pull/656
    - name: VMware VM
      community.vmware.vmware_guest:
        hostname: "{{ vcenter_ip }}"
        username: "{{ vcenter_username }}"
        password: "{{ vcenter_password }}"
        folder: /
        datacenter: Metal
        name: sno_vm
        state: present
        guest_id: rhel8_64Guest
        disk:
          - size_gb: 180
            type: thin
            datastore: datastore1
        networks:
          - name: VM Public Net 1
            start_connected: yes
        cdrom:
          - controller_number: 0
            unit_number: 0
            state: present
            type: iso
            iso_path: "[datastore1] {{ cluster_name }}.iso"
        hardware:
          memory_mb: 32768
          num_cpus: 8
          scsi: paravirtual
          boot_firmware: "efi"
        advanced_settings:
          - key: "pciPassthru.use64bitMMIO"
            value: "TRUE"
          - key: "pciPassthru.64bitMMIOSizeGB"
            value: "512"
        customization:
          domain: sno
          hostname: sno
        wait_for_ip_address: yes
        validate_certs: no
      register: vm
      when: create_vm

    - name: Wait for host(s) in the cluster
      ai_cluster_info:
        name: "{{ cluster_name }}"
        offlinetoken: "{{ ocm_offline_token }}"
      register: sno_cluster
      until: sno_cluster.hosts | length == 1
      retries: 10
      delay: 60

    - name: Set SNO hostname (instead of localhost)
      ai_host:
        name: "{{ sno_cluster.hosts[0].id }}"
        state: present
        parameters:
          requested_hostname: sno
        offlinetoken: "{{ ocm_offline_token }}"
      when: sno_cluster.status == 'pending-for-input'

    - name: Start SNO installation
      ai_cluster:
        name: "{{ cluster_name }}"
        state: started
        offlinetoken: "{{ ocm_offline_token }}"
      when: sno_cluster.status != 'installed'

    - name: Verify SNO installation
      ai_cluster:
        name: "{{ cluster_name }}"
        state: installed
        offlinetoken: "{{ ocm_offline_token }}"

    - name: Set NGC credentials
      ansible.builtin.set_fact:
        ngc_credentials: "$oathtoken:{{ ngc_api_key }}"

    - name: Set NGC pull secret auth
      ansible.builtin.set_fact:
        ngc_auth: "{\"auths\":{\"nvcr.io/nvaie\":{\"username\":\"$oauthtoken\",\"password\":\"{{ ngc_api_key }}\",\"auth\": \"{{ ngc_credentials | b64encode }}\",\"email\":\"{{ ngc_email }}\"}}}"

    - name: Download cluster's kubeconfig
      ansible.builtin.command: "aicli --token {{ ocm_offline_token }} download kubeconfig --path tmp/ {{ sno_cluster.id }}"

    - name: Create vGPU config resources
      kubernetes.core.k8s:
        host: "https://{{ sno_host }}:6443" # use host in case there's no DNS entry
        kubeconfig: "tmp/kubeconfig.{{ sno_cluster.id }}"
        state: present
        template:
          - ngc-secret.yaml.j2
          - licensing-config.yaml.j2
        validate_certs: false
        validate:
          fail_on_error: yes

    - name: Install NFD operator
      # https://docs.openshift.com/container-platform/4.10/hardware_enablement/psap-node-feature-discovery-operator.html
      # https://github.com/openshift/cluster-nfd-operator
      kubernetes.core.k8s:
        host: "https://{{ sno_host }}:6443"
        kubeconfig: "tmp/kubeconfig.{{ sno_cluster.id }}"
        state: present
        src: nfd-operator.yaml
        validate_certs: false
        validate:
          fail_on_error: yes

    - name: Wait for NFD operator pods to start
      kubernetes.core.k8s_info:
        host: "https://{{ sno_host }}:6443"
        kubeconfig: "tmp/kubeconfig.{{ sno_cluster.id }}"
        kind: Pod
        namespace: openshift-nfd
        wait: true
        wait_condition:
          type: Ready
        validate_certs: false
      register: nfd_pods
      until: nfd_pods | length > 0

    - name: Create NFD instance
      kubernetes.core.k8s:
        host: "https://{{ sno_host }}:6443"
        kubeconfig: "tmp/kubeconfig.{{ cluster_name }}"
        state: present
        src: nfd-instance.yaml
        validate_certs: false
        validate:
          fail_on_error: yes

    - name: Wait for NFD instance pods to start
      kubernetes.core.k8s_info:
        host: "https://{{ sno_host }}:6443"
        kubeconfig: "tmp/kubeconfig.{{ cluster_name }}"
        kind: Pod
        namespace: openshift-nfd
        wait: true
        wait_condition:
          type: Ready
        validate_certs: false
      register: nfd_instance_pods
      until: nfd_instance_pods | length > nfd_pods | length

    # - name: Install GPU operator
    #   kubernetes.core.k8s:
    #     host: "https://{{ sno_host }}:6443"
    #     kubeconfig: "tmp/kubeconfig.{{ cluster_name }}"
    #     state: present
    #     template: gpu-operator.yaml
    #     validate_certs: false
    #     validate:
    #       fail_on_error: yes
